%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/
% Encoding: UTF-8

@article{Luca,
  author  = {Luca Pegolotti and Martin R. Pfaller and Natalia L. Rubio and Ke Ding and Rita Brugarolas Brufau and Eric Darve and Alison L. Marsden}, 
   title  = {Learning Reduced-Order Models for Cardiovascular Simulations with Graph Neural Networks},
  journal = {Computers in Biology and Medicine},
  year    = 2023
}

@article{MattiaCorti,
  author  = {Corti, Mattia and Bonizzoni, Francesca and Dede', Luca and Quarteroni, Alfio M. and Antonietti, Paola F.},
  title   = {Discontinuous Galerkin Methods for Fisher-Kolmogorov Equation with Application to alpha-Synuclein Spreading in Parkinson's Disease},
  journal = {Computer Methods in Applied Mechanics and Engineering},
  year    = 2023
}

@article{meshgraphnet,
  author  = {Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter W. Battaglia},
  title   = {Learning Mesh-Based Simulation with Graph Networks},
  year    = 2021
}

# cite this website https://fenicsproject.org/olddocs/dolfin/2019.1.0/python/

@online{FEniCS,
  author       = {FEniCS Team}, 
  title        = {FEniCS Documentation},
  year         = 2019,
  url          = {https://fenicsproject.org/olddocs/dolfin/2019.1.0/python/},
  urldate      = {2024-02-01}
}

 @article{Zhou_Cui_Hu_Zhang_Yang_Liu_Wang_Li_Sun_2021, title={Graph Neural Networks: A Review of Methods and Applications}, url={http://arxiv.org/abs/1812.08434}, DOI={10.48550/arXiv.1812.08434}, abstractNote={Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.}, note={arXiv:1812.08434 [cs, stat]}, number={arXiv:1812.08434}, publisher={arXiv}, author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong}, year={2021}, month=oct }

 @book{Formaggia_Quarteroni_Veneziani_2009, address={Milano}, title={Cardiovascular Mathematics}, ISBN={978-88-470-1151-9}, url={http://link.springer.com/10.1007/978-88-470-1152-6}, DOI={10.1007/978-88-470-1152-6}, publisher={Springer Milan}, year={2009}, language={en} }


 @article{Kingma_Ba_2017, title={Adam: A Method for Stochastic Optimization}, url={http://arxiv.org/abs/1412.6980}, DOI={10.48550/arXiv.1412.6980}, abstractNote={We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}, note={arXiv:1412.6980 [cs]}, number={arXiv:1412.6980}, publisher={arXiv}, author={Kingma, Diederik P. and Ba, Jimmy}, year={2017}, month=jan }
